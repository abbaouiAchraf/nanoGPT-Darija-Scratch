{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-21T14:14:57.802112Z"
    },
    "id": "b7VSlYUlN0z7",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install datasets --quiet\n",
    "%pip install git+https://github.com/huggingface/transformers --quiet\n",
    "%pip install git+https://github.com/huggingface/accelerate --quiet\n",
    "%pip install git+https://github.com/huggingface/peft --quiet\n",
    "%pip install bitsandbytes --quiet\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nZSwNruDhM6o",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "DATASET = \"DRAGOO/dataset_dyal_darija\"  \n",
    "LIMIT_SIZE = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### Downloading the darija dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "CY28xJh-hSCQ",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "9b778f45-6544-4dbc-cb22-db1c51cfb085"
   },
   "outputs": [],
   "source": [
    "data = load_dataset(DATASET)\n",
    "text = data['train']['text'][:LIMIT_SIZE]\n",
    "\n",
    "with open('cleaned_data.txt', 'w', encoding='utf-8') as f:\n",
    "  f.write('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "l1P1Lp8Dhdg2",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "0ee9c94b-3689-47cc-ede4-a50506629102"
   },
   "outputs": [],
   "source": [
    "with open('cleaned_data.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wCsTTaEXhT_o",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character count in dataset: 37440616\n"
     ]
    }
   ],
   "source": [
    "print(f'Character count in dataset: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WI1d_-jYiF6H",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: \n",
      "   ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~    ¤ ¥ © « ¬ ­ ° ² · » ¿ À Ç É Ö × à á â ç è é ê í î ï ñ ò ó ô ö ù ú û ü ğ ı ş ͠ ב ג ח ט י ע ת ، ؛ ؜ ؟ ء آ أ ؤ إ ئ ا ب ة ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ـ ف ق ك ل م ن ه و ى ي ً ٌ ٍ َ ُ ِ ّ ْ ٓ ٔ ٕ ٠ ١ ٢ ٣ ٤ ٥ ٦ ٧ ٨ ٩ ٪ ٭ ٰ ٱ پ ځ ڄ چ ډ ڜ ڤ ڨ ک ڭ گ ہ ۃ ۅ ی ۔ ۗ ۚ ۣ ݣ ߘ ‌ ‍ ‎ ‏ – ‘ ’ “ ” „ • … ‫ ‬ ‼ ⁉ ⁦ ⁧ ⁩ → ↘ ↙ ↩ ↪ √ ∞ ⏩ ⏪ ⏭ ⏯ ⏰ ⏳ ⏸ ■ ▶ ● ◻ ◽ ☀ ☁ ★ ☆ ☉ ☘ ☝ ☹ ☺ ♀ ♂ ♠ ♡ ♣ ♤ ♥ ♦ ♧ ♨ ♪ ♬ ♾ ⚘ ⚜ ⚡ ⛅ ⛔ ⛼ ✅ ✈ ✋ ✌ ✍ ✏ ✒ ✔ ✨ ✳ ❄ ❌ ❗ ❤ ➖ ⬅ ⬇ ⭐ ⭕ 《 》 〽 ﮓ ﴾ ﴿ ️ ﺀ ﺁ ﺂ ﺃ ﺄ ﺆ ﺇ ﺊ ﺋ ﺌ ﺍ ﺎ ﺏ ﺐ ﺑ ﺒ ﺓ ﺔ ﺕ ﺖ ﺗ ﺘ ﺙ ﺚ ﺛ ﺜ ﺝ ﺞ ﺟ ﺠ ﺡ ﺢ ﺣ ﺤ ﺥ ﺧ ﺨ ﺩ ﺪ ﺫ ﺬ ﺭ ﺮ ﺯ ﺰ ﺱ ﺲ ﺳ ﺴ ﺵ ﺶ ﺷ ﺸ ﺹ ﺺ ﺻ ﺼ ﺽ ﺾ ﺿ ﻀ ﻁ ﻂ ﻃ ﻄ ﻈ ﻉ ﻊ ﻋ ﻌ ﻍ ﻏ ﻐ ﻑ ﻒ ﻓ ﻔ ﻕ ﻖ ﻗ ﻘ ﻙ ﻚ ﻛ ﻜ ﻝ ﻞ ﻟ ﻠ ﻡ ﻢ ﻣ ﻤ ﻥ ﻦ ﻧ ﻨ ﻩ ﻪ ﻫ ﻬ ﻭ ﻮ ﻯ ﻰ ﻱ ﻲ ﻳ ﻴ ﻷ ﻹ ﻻ ﻼ ﻿ 🇦 🇲 🇵 🇸 🇺 🌃 🌄 🌅 🌆 🌊 🌌 🌏 🌑 🌒 🌓 🌗 🌘 🌙 🌚 🌝 🌞 🌟 🌠 🌤 🌥 🌱 🌷 🌸 🌹 🌺 🌻 🌼 🌾 🌿 🍀 🍁 🍂 🍃 🍆 🍇 🍋 🍑 🍓 🍭 🍯 🍰 🍷 🍾 🎀 🎁 🎂 🎄 🎇 🎈 🎉 🎑 🎗 🎤 🎬 🎭 🎵 🎶 🎺 🎻 🎼 🏙 🏠 🏢 🏭 🏮 🏻 🏽 🏿 🐈 🐍 🐓 🐥 🐳 🐸 🐻 👀 👁 👂 👃 👄 👇 👈 👉 👊 👋 👌 👍 👏 👐 👑 👔 👕 👖 👙 👞 👠 👣 👦 👨 👪 👫 👮 👰 👴 👵 👶 👹 👽 👿 💀 💁 💃 💄 💆 💇 💉 💊 💋 💌 💍 💎 💏 💐 💑 💓 💔 💕 💖 💘 💙 💚 💛 💜 💝 💞 💠 💡 💣 💤 💥 💦 💨 💪 💫 💬 💭 💽 📋 📎 📗 📙 📚 📝 📞 📟 📣 📧 📨 📩 📰 📱 📸 📻 🔔 🔙 🔚 🔛 🔜 🔞 🔥 🔪 🔫 🔱 🔴 🔶 🔷 🔸 🕊 🕒 🕔 🕖 🕢 🕦 🕪 🕯 🕳 🖊 🖋 🖍 🖐 🖕 🖖 🖤 🗡 🗼 🗿 😀 😁 😂 😃 😄 😅 😆 😇 😈 😉 😊 😋 😌 😍 😎 😏 😐 😑 😒 😓 😔 😕 😖 😗 😘 😙 😚 😛 😜 😝 😞 😟 😠 😡 😢 😣 😤 😥 😦 😧 😨 😩 😪 😫 😬 😭 😮 😯 😰 😱 😲 😳 😴 😵 😶 😷 😹 😺 😻 😿 🙀 🙁 🙂 🙃 🙄 🙅 🙆 🙇 🙈 🙉 🙊 🙌 🙏 🚒 🚨 🚫 🚶 🛡 🛢 🛫 🛬 🤐 🤓 🤔 🤕 🤗 🤚 🤝 🤞 🤢 🤣 🤤 🤥 🤦 🤨 🤩 🤫 🤬 🤭 🤴 🤵 🤷 🥀 🥕 🥰 🥺 🦄 🧐 🧝 🧡 🧿 🪵\n",
      "Vocab size: 750\n"
     ]
    }
   ],
   "source": [
    "# Calculate vocab_size\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('Unique characters: {count}'.format(count=' '.join(chars)))\n",
    "print(f'Vocab size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char2idx and idx2char\n",
    "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [char2idx[char] for char in x]\n",
    "decode = lambda x: ''.join([idx2char[idx] for idx in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: السلام عليكم ورحمة الله وبركاته\n",
      "Encoded text: [154, 178, 166, 178, 154, 179, 1, 172, 178, 184, 177, 179, 1, 182, 164, 160, 179, 156, 1, 154, 178, 178, 181, 1, 182, 155, 164, 177, 154, 157, 181]\n",
      "Decoded text: السلام عليكم ورحمة الله وبركاته\n"
     ]
    }
   ],
   "source": [
    "sentence = \"السلام عليكم ورحمة الله وبركاته\"\n",
    "encoded_text = encode(sentence)\n",
    "decoded_text = decode(encoded_text)\n",
    "\n",
    "print(f'Original text: {sentence}')\n",
    "print(f'Encoded text: {encoded_text}')\n",
    "print(f'Decoded text: {decoded_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tiktoken by openai to use sub-word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([37440616])\n"
     ]
    }
   ],
   "source": [
    "# Transforme data to tensor\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(f'Tensor shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor data: tensor([180, 154, 166,   1, 155, 162, 154, 182,   1, 157, 184, 157, 155, 154,\n",
      "        177, 154, 182,   1, 182, 155, 162, 184, 157,   1, 157, 180, 181, 162,\n",
      "        164,   1, 155,   1, 180, 175, 172, 178,   1, 155, 160, 154, 178,   1,\n",
      "        152, 178, 183,   1, 157, 180, 181, 162, 184,   1, 154, 178, 180, 154,\n",
      "        166,   0, 180, 154, 166,   1, 155, 162, 154, 182,   1, 157, 184, 157,\n",
      "        155, 154, 177, 154, 182,   1, 182, 155, 162, 184, 157,   1, 157, 180,\n",
      "        181, 162, 164,   1, 155,   1, 180, 175, 172, 178,   1, 155, 160, 154,\n",
      "        178,   1, 152, 178, 183,   1, 157, 180, 181, 162, 184,   1, 154, 178,\n",
      "        180, 154, 166,   0, 155, 177, 184, 157,   1, 182, 154, 178, 180, 154,\n",
      "        166,   1, 177, 178, 181, 179,   1, 166, 177, 157, 182,   1, 182, 154,\n",
      "        180, 154,   1, 159, 154, 180, 184,   1, 178, 172, 159, 155,   1, 175,\n",
      "        164, 154, 166, 184,   1, 155, 160, 154, 178,   1, 152, 178, 183,   1,\n",
      "        177, 180, 160, 178, 179,   0, 180, 154, 169,   1, 177, 178, 167, 184,\n",
      "          1, 177, 184, 166, 178, 179,   1, 172, 178, 184, 154,   1, 150, 182,\n",
      "          1, 177, 184, 181, 162, 164, 182,   1, 179, 172, 154, 184, 154,   1,\n",
      "        180, 168,   1, 155, 177, 183,   1, 182, 180, 168,   1, 178, 154, 161,\n",
      "        164,   1, 179, 154, 175, 181, 179, 157, 182, 167,   0, 157, 172, 164,\n",
      "        162, 180, 154,   1, 178, 172, 167, 154,   1, 182,   1, 177, 154, 180,\n",
      "          1, 178, 172, 167, 154,   1, 177, 154, 175, 184,   1, 178, 177, 178,\n",
      "        167, 184,   1, 154, 178, 179, 172, 164, 182, 162, 184, 180,   0, 162,\n",
      "        182, 165, 180, 154,   1, 184, 154, 179, 154, 157,   1, 165, 182, 184,\n",
      "        180, 156,   1, 179, 154,   1, 172, 179, 164, 184,   1, 179, 154,   1,\n",
      "        180, 180, 166, 154, 181, 154,   0, 177, 179, 178, 157,   1, 176, 164,\n",
      "        154, 184, 157, 184,   1, 175, 184,   1, 154, 178, 179, 172, 181, 162,\n",
      "          1, 155, 165, 165,   0, 179, 154, 177, 180, 157, 167,   1, 177, 154,\n",
      "        180,   1, 176, 164, 154,   1, 179, 165, 184, 154, 180,   0, 161, 162,\n",
      "        179, 157,   1, 175, 184,   1, 182, 154, 160, 162,   1, 179, 180,   1,\n",
      "        154, 178, 166, 155, 184, 170, 154, 164, 154, 157,   1, 178, 184,   1,\n",
      "        176, 164, 154, 155,   1, 179, 180,   1, 154, 178, 162, 154, 164,   1,\n",
      "        154, 178, 160, 179, 162,   1, 178, 178, 181,   1, 150, 179, 182, 164,\n",
      "        184,   1, 179, 157, 184, 166, 164, 156,   1, 182,   1, 177, 180, 172,\n",
      "        184, 167,   1, 179, 172,   1, 182, 154, 178, 162, 184, 183,   0, 176,\n",
      "        164, 164, 157,   1, 180, 159, 179, 172,   1, 154, 178, 168, 162, 154,\n",
      "        176,   1, 178, 179, 164, 154, 157, 184,   1, 177, 154, 180, 157,   1,\n",
      "        154, 178, 161, 162, 179, 156,   1, 173, 154, 162, 156,   1, 179, 165,\n",
      "        184, 154, 180,   1, 182,   1, 160, 184, 157,   1])\n"
     ]
    }
   ],
   "source": [
    "print(f'Tensor data: {data[:500]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor data: ناس بداو تيتباكاو وبديت تنهدر ب نفعل بحال إلى تنهدي الناس\n",
      "ناس بداو تيتباكاو وبديت تنهدر ب نفعل بحال إلى تنهدي الناس\n",
      "بكيت والناس كلهم سكتو وانا جاني لعجب فراسي بحال إلى كنحلم\n",
      "ناض كلشي كيسلم عليا أو كيهدرو معايا نص بكى ونص لاخر مافهمتوش\n",
      "تعردنا لعشا و كان لعشا كافي لكلشي المعرودين\n",
      "دوزنا يامات زوينة ما عمري ما ننساها\n",
      "كملت قرايتي في المعهد بزز\n",
      "ماكنتش كان قرا مزيان\n",
      "خدمت في واحد من السبيطارات لي قراب من الدار الحمد لله أموري متيسرة و كنعيش مع والديى\n",
      "قررت نجمع الصداق لمراتي كانت الخدمة غادة مزيان و حيت \n"
     ]
    }
   ],
   "source": [
    "s = data[:500]\n",
    "# convert tensor to array\n",
    "s = s.numpy()\n",
    "print(f'Tensor data: {decode(s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
