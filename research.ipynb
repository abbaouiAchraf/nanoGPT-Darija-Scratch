{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-07-21T14:14:57.802112Z"
    },
    "id": "b7VSlYUlN0z7",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install datasets --quiet\n",
    "%pip install git+https://github.com/huggingface/transformers --quiet\n",
    "%pip install git+https://github.com/huggingface/accelerate --quiet\n",
    "%pip install git+https://github.com/huggingface/peft --quiet\n",
    "%pip install bitsandbytes --quiet\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "nZSwNruDhM6o",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "DATASET = \"DRAGOO/dataset_dyal_darija\"  \n",
    "LIMIT_SIZE = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### Downloading the darija dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 219
    },
    "id": "CY28xJh-hSCQ",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "9b778f45-6544-4dbc-cb22-db1c51cfb085"
   },
   "outputs": [],
   "source": [
    "data = load_dataset(DATASET)\n",
    "text = data['train']['text'][:LIMIT_SIZE]\n",
    "\n",
    "with open('cleaned_data.txt', 'w', encoding='utf-8') as f:\n",
    "  f.write('\\n'.join(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "l1P1Lp8Dhdg2",
    "jupyter": {
     "is_executing": true
    },
    "outputId": "0ee9c94b-3689-47cc-ede4-a50506629102"
   },
   "outputs": [],
   "source": [
    "with open('cleaned_data.txt', 'r', encoding='utf-8') as f:\n",
    "  text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wCsTTaEXhT_o",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character count in dataset: 37440616\n"
     ]
    }
   ],
   "source": [
    "print(f'Character count in dataset: {len(text)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WI1d_-jYiF6H",
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique characters: \n",
      "   ! \" # $ % & ' ( ) * + , - . / 0 1 2 3 4 5 6 7 8 9 : ; < = > ? @ A B C D E F G H I J K L M N O P Q R S T U V W X Y Z [ \\ ] ^ _ ` a b c d e f g h i j k l m n o p q r s t u v w x y z { | } ~  Â  Â¤ Â¥ Â© Â« Â¬ Â­ Â° Â² Â· Â» Â¿ Ã€ Ã‡ Ã‰ Ã– Ã— Ã  Ã¡ Ã¢ Ã§ Ã¨ Ã© Ãª Ã­ Ã® Ã¯ Ã± Ã² Ã³ Ã´ Ã¶ Ã¹ Ãº Ã» Ã¼ ÄŸ Ä± ÅŸ Í  ×‘ ×’ ×— ×˜ ×™ ×¢ ×ª ØŒ Ø› Øœ ØŸ Ø¡ Ø¢ Ø£ Ø¤ Ø¥ Ø¦ Ø§ Ø¨ Ø© Øª Ø« Ø¬ Ø­ Ø® Ø¯ Ø° Ø± Ø² Ø³ Ø´ Øµ Ø¶ Ø· Ø¸ Ø¹ Øº Ù€ Ù Ù‚ Ùƒ Ù„ Ù… Ù† Ù‡ Ùˆ Ù‰ ÙŠ Ù‹ ÙŒ Ù Ù Ù Ù Ù‘ Ù’ Ù“ Ù” Ù• Ù  Ù¡ Ù¢ Ù£ Ù¤ Ù¥ Ù¦ Ù§ Ù¨ Ù© Ùª Ù­ Ù° Ù± Ù¾ Ú Ú„ Ú† Ú‰ Úœ Ú¤ Ú¨ Ú© Ú­ Ú¯ Û Ûƒ Û… ÛŒ Û” Û— Ûš Û£ İ£ ß˜ â€Œ â€ â€ â€ â€“ â€˜ â€™ â€œ â€ â€ â€¢ â€¦ â€« â€¬ â€¼ â‰ â¦ â§ â© â†’ â†˜ â†™ â†© â†ª âˆš âˆ â© âª â­ â¯ â° â³ â¸ â–  â–¶ â— â—» â—½ â˜€ â˜ â˜… â˜† â˜‰ â˜˜ â˜ â˜¹ â˜º â™€ â™‚ â™  â™¡ â™£ â™¤ â™¥ â™¦ â™§ â™¨ â™ª â™¬ â™¾ âš˜ âšœ âš¡ â›… â›” â›¼ âœ… âœˆ âœ‹ âœŒ âœ âœ âœ’ âœ” âœ¨ âœ³ â„ âŒ â— â¤ â– â¬… â¬‡ â­ â­• ã€Š ã€‹ ã€½ ï®“ ï´¾ ï´¿ ï¸ ïº€ ïº ïº‚ ïºƒ ïº„ ïº† ïº‡ ïºŠ ïº‹ ïºŒ ïº ïº ïº ïº ïº‘ ïº’ ïº“ ïº” ïº• ïº– ïº— ïº˜ ïº™ ïºš ïº› ïºœ ïº ïº ïºŸ ïº  ïº¡ ïº¢ ïº£ ïº¤ ïº¥ ïº§ ïº¨ ïº© ïºª ïº« ïº¬ ïº­ ïº® ïº¯ ïº° ïº± ïº² ïº³ ïº´ ïºµ ïº¶ ïº· ïº¸ ïº¹ ïºº ïº» ïº¼ ïº½ ïº¾ ïº¿ ï»€ ï» ï»‚ ï»ƒ ï»„ ï»ˆ ï»‰ ï»Š ï»‹ ï»Œ ï» ï» ï» ï»‘ ï»’ ï»“ ï»” ï»• ï»– ï»— ï»˜ ï»™ ï»š ï»› ï»œ ï» ï» ï»Ÿ ï»  ï»¡ ï»¢ ï»£ ï»¤ ï»¥ ï»¦ ï»§ ï»¨ ï»© ï»ª ï»« ï»¬ ï»­ ï»® ï»¯ ï»° ï»± ï»² ï»³ ï»´ ï»· ï»¹ ï»» ï»¼ ï»¿ ğŸ‡¦ ğŸ‡² ğŸ‡µ ğŸ‡¸ ğŸ‡º ğŸŒƒ ğŸŒ„ ğŸŒ… ğŸŒ† ğŸŒŠ ğŸŒŒ ğŸŒ ğŸŒ‘ ğŸŒ’ ğŸŒ“ ğŸŒ— ğŸŒ˜ ğŸŒ™ ğŸŒš ğŸŒ ğŸŒ ğŸŒŸ ğŸŒ  ğŸŒ¤ ğŸŒ¥ ğŸŒ± ğŸŒ· ğŸŒ¸ ğŸŒ¹ ğŸŒº ğŸŒ» ğŸŒ¼ ğŸŒ¾ ğŸŒ¿ ğŸ€ ğŸ ğŸ‚ ğŸƒ ğŸ† ğŸ‡ ğŸ‹ ğŸ‘ ğŸ“ ğŸ­ ğŸ¯ ğŸ° ğŸ· ğŸ¾ ğŸ€ ğŸ ğŸ‚ ğŸ„ ğŸ‡ ğŸˆ ğŸ‰ ğŸ‘ ğŸ— ğŸ¤ ğŸ¬ ğŸ­ ğŸµ ğŸ¶ ğŸº ğŸ» ğŸ¼ ğŸ™ ğŸ  ğŸ¢ ğŸ­ ğŸ® ğŸ» ğŸ½ ğŸ¿ ğŸˆ ğŸ ğŸ“ ğŸ¥ ğŸ³ ğŸ¸ ğŸ» ğŸ‘€ ğŸ‘ ğŸ‘‚ ğŸ‘ƒ ğŸ‘„ ğŸ‘‡ ğŸ‘ˆ ğŸ‘‰ ğŸ‘Š ğŸ‘‹ ğŸ‘Œ ğŸ‘ ğŸ‘ ğŸ‘ ğŸ‘‘ ğŸ‘” ğŸ‘• ğŸ‘– ğŸ‘™ ğŸ‘ ğŸ‘  ğŸ‘£ ğŸ‘¦ ğŸ‘¨ ğŸ‘ª ğŸ‘« ğŸ‘® ğŸ‘° ğŸ‘´ ğŸ‘µ ğŸ‘¶ ğŸ‘¹ ğŸ‘½ ğŸ‘¿ ğŸ’€ ğŸ’ ğŸ’ƒ ğŸ’„ ğŸ’† ğŸ’‡ ğŸ’‰ ğŸ’Š ğŸ’‹ ğŸ’Œ ğŸ’ ğŸ’ ğŸ’ ğŸ’ ğŸ’‘ ğŸ’“ ğŸ’” ğŸ’• ğŸ’– ğŸ’˜ ğŸ’™ ğŸ’š ğŸ’› ğŸ’œ ğŸ’ ğŸ’ ğŸ’  ğŸ’¡ ğŸ’£ ğŸ’¤ ğŸ’¥ ğŸ’¦ ğŸ’¨ ğŸ’ª ğŸ’« ğŸ’¬ ğŸ’­ ğŸ’½ ğŸ“‹ ğŸ“ ğŸ“— ğŸ“™ ğŸ“š ğŸ“ ğŸ“ ğŸ“Ÿ ğŸ“£ ğŸ“§ ğŸ“¨ ğŸ“© ğŸ“° ğŸ“± ğŸ“¸ ğŸ“» ğŸ”” ğŸ”™ ğŸ”š ğŸ”› ğŸ”œ ğŸ” ğŸ”¥ ğŸ”ª ğŸ”« ğŸ”± ğŸ”´ ğŸ”¶ ğŸ”· ğŸ”¸ ğŸ•Š ğŸ•’ ğŸ•” ğŸ•– ğŸ•¢ ğŸ•¦ ğŸ•ª ğŸ•¯ ğŸ•³ ğŸ–Š ğŸ–‹ ğŸ– ğŸ– ğŸ–• ğŸ–– ğŸ–¤ ğŸ—¡ ğŸ—¼ ğŸ—¿ ğŸ˜€ ğŸ˜ ğŸ˜‚ ğŸ˜ƒ ğŸ˜„ ğŸ˜… ğŸ˜† ğŸ˜‡ ğŸ˜ˆ ğŸ˜‰ ğŸ˜Š ğŸ˜‹ ğŸ˜Œ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜ ğŸ˜‘ ğŸ˜’ ğŸ˜“ ğŸ˜” ğŸ˜• ğŸ˜– ğŸ˜— ğŸ˜˜ ğŸ˜™ ğŸ˜š ğŸ˜› ğŸ˜œ ğŸ˜ ğŸ˜ ğŸ˜Ÿ ğŸ˜  ğŸ˜¡ ğŸ˜¢ ğŸ˜£ ğŸ˜¤ ğŸ˜¥ ğŸ˜¦ ğŸ˜§ ğŸ˜¨ ğŸ˜© ğŸ˜ª ğŸ˜« ğŸ˜¬ ğŸ˜­ ğŸ˜® ğŸ˜¯ ğŸ˜° ğŸ˜± ğŸ˜² ğŸ˜³ ğŸ˜´ ğŸ˜µ ğŸ˜¶ ğŸ˜· ğŸ˜¹ ğŸ˜º ğŸ˜» ğŸ˜¿ ğŸ™€ ğŸ™ ğŸ™‚ ğŸ™ƒ ğŸ™„ ğŸ™… ğŸ™† ğŸ™‡ ğŸ™ˆ ğŸ™‰ ğŸ™Š ğŸ™Œ ğŸ™ ğŸš’ ğŸš¨ ğŸš« ğŸš¶ ğŸ›¡ ğŸ›¢ ğŸ›« ğŸ›¬ ğŸ¤ ğŸ¤“ ğŸ¤” ğŸ¤• ğŸ¤— ğŸ¤š ğŸ¤ ğŸ¤ ğŸ¤¢ ğŸ¤£ ğŸ¤¤ ğŸ¤¥ ğŸ¤¦ ğŸ¤¨ ğŸ¤© ğŸ¤« ğŸ¤¬ ğŸ¤­ ğŸ¤´ ğŸ¤µ ğŸ¤· ğŸ¥€ ğŸ¥• ğŸ¥° ğŸ¥º ğŸ¦„ ğŸ§ ğŸ§ ğŸ§¡ ğŸ§¿ ğŸªµ\n",
      "Vocab size: 750\n"
     ]
    }
   ],
   "source": [
    "# Calculate vocab_size\n",
    "chars = sorted(list(set(text)))\n",
    "vocab_size = len(chars)\n",
    "print('Unique characters: {count}'.format(count=' '.join(chars)))\n",
    "print(f'Vocab size: {vocab_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create char2idx and idx2char\n",
    "char2idx = {char: idx for idx, char in enumerate(chars)}\n",
    "idx2char = {idx: char for idx, char in enumerate(chars)}\n",
    "\n",
    "encode = lambda x: [char2idx[char] for char in x]\n",
    "decode = lambda x: ''.join([idx2char[idx] for idx in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡\n",
      "Encoded text: [154, 178, 166, 178, 154, 179, 1, 172, 178, 184, 177, 179, 1, 182, 164, 160, 179, 156, 1, 154, 178, 178, 181, 1, 182, 155, 164, 177, 154, 157, 181]\n",
      "Decoded text: Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Ø§Ù„Ø³Ù„Ø§Ù… Ø¹Ù„ÙŠÙƒÙ… ÙˆØ±Ø­Ù…Ø© Ø§Ù„Ù„Ù‡ ÙˆØ¨Ø±ÙƒØ§ØªÙ‡\"\n",
    "encoded_text = encode(sentence)\n",
    "decoded_text = decode(encoded_text)\n",
    "\n",
    "print(f'Original text: {sentence}')\n",
    "print(f'Encoded text: {encoded_text}')\n",
    "print(f'Decoded text: {decoded_text}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use tiktoken by openai to use sub-word tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor shape: torch.Size([37440616])\n"
     ]
    }
   ],
   "source": [
    "# Transforme data to tensor\n",
    "\n",
    "data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(f'Tensor shape: {data.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor data: tensor([180, 154, 166,   1, 155, 162, 154, 182,   1, 157, 184, 157, 155, 154,\n",
      "        177, 154, 182,   1, 182, 155, 162, 184, 157,   1, 157, 180, 181, 162,\n",
      "        164,   1, 155,   1, 180, 175, 172, 178,   1, 155, 160, 154, 178,   1,\n",
      "        152, 178, 183,   1, 157, 180, 181, 162, 184,   1, 154, 178, 180, 154,\n",
      "        166,   0, 180, 154, 166,   1, 155, 162, 154, 182,   1, 157, 184, 157,\n",
      "        155, 154, 177, 154, 182,   1, 182, 155, 162, 184, 157,   1, 157, 180,\n",
      "        181, 162, 164,   1, 155,   1, 180, 175, 172, 178,   1, 155, 160, 154,\n",
      "        178,   1, 152, 178, 183,   1, 157, 180, 181, 162, 184,   1, 154, 178,\n",
      "        180, 154, 166,   0, 155, 177, 184, 157,   1, 182, 154, 178, 180, 154,\n",
      "        166,   1, 177, 178, 181, 179,   1, 166, 177, 157, 182,   1, 182, 154,\n",
      "        180, 154,   1, 159, 154, 180, 184,   1, 178, 172, 159, 155,   1, 175,\n",
      "        164, 154, 166, 184,   1, 155, 160, 154, 178,   1, 152, 178, 183,   1,\n",
      "        177, 180, 160, 178, 179,   0, 180, 154, 169,   1, 177, 178, 167, 184,\n",
      "          1, 177, 184, 166, 178, 179,   1, 172, 178, 184, 154,   1, 150, 182,\n",
      "          1, 177, 184, 181, 162, 164, 182,   1, 179, 172, 154, 184, 154,   1,\n",
      "        180, 168,   1, 155, 177, 183,   1, 182, 180, 168,   1, 178, 154, 161,\n",
      "        164,   1, 179, 154, 175, 181, 179, 157, 182, 167,   0, 157, 172, 164,\n",
      "        162, 180, 154,   1, 178, 172, 167, 154,   1, 182,   1, 177, 154, 180,\n",
      "          1, 178, 172, 167, 154,   1, 177, 154, 175, 184,   1, 178, 177, 178,\n",
      "        167, 184,   1, 154, 178, 179, 172, 164, 182, 162, 184, 180,   0, 162,\n",
      "        182, 165, 180, 154,   1, 184, 154, 179, 154, 157,   1, 165, 182, 184,\n",
      "        180, 156,   1, 179, 154,   1, 172, 179, 164, 184,   1, 179, 154,   1,\n",
      "        180, 180, 166, 154, 181, 154,   0, 177, 179, 178, 157,   1, 176, 164,\n",
      "        154, 184, 157, 184,   1, 175, 184,   1, 154, 178, 179, 172, 181, 162,\n",
      "          1, 155, 165, 165,   0, 179, 154, 177, 180, 157, 167,   1, 177, 154,\n",
      "        180,   1, 176, 164, 154,   1, 179, 165, 184, 154, 180,   0, 161, 162,\n",
      "        179, 157,   1, 175, 184,   1, 182, 154, 160, 162,   1, 179, 180,   1,\n",
      "        154, 178, 166, 155, 184, 170, 154, 164, 154, 157,   1, 178, 184,   1,\n",
      "        176, 164, 154, 155,   1, 179, 180,   1, 154, 178, 162, 154, 164,   1,\n",
      "        154, 178, 160, 179, 162,   1, 178, 178, 181,   1, 150, 179, 182, 164,\n",
      "        184,   1, 179, 157, 184, 166, 164, 156,   1, 182,   1, 177, 180, 172,\n",
      "        184, 167,   1, 179, 172,   1, 182, 154, 178, 162, 184, 183,   0, 176,\n",
      "        164, 164, 157,   1, 180, 159, 179, 172,   1, 154, 178, 168, 162, 154,\n",
      "        176,   1, 178, 179, 164, 154, 157, 184,   1, 177, 154, 180, 157,   1,\n",
      "        154, 178, 161, 162, 179, 156,   1, 173, 154, 162, 156,   1, 179, 165,\n",
      "        184, 154, 180,   1, 182,   1, 160, 184, 157,   1])\n"
     ]
    }
   ],
   "source": [
    "print(f'Tensor data: {data[:500]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor data: Ù†Ø§Ø³ Ø¨Ø¯Ø§Ùˆ ØªÙŠØªØ¨Ø§ÙƒØ§Ùˆ ÙˆØ¨Ø¯ÙŠØª ØªÙ†Ù‡Ø¯Ø± Ø¨ Ù†ÙØ¹Ù„ Ø¨Ø­Ø§Ù„ Ø¥Ù„Ù‰ ØªÙ†Ù‡Ø¯ÙŠ Ø§Ù„Ù†Ø§Ø³\n",
      "Ù†Ø§Ø³ Ø¨Ø¯Ø§Ùˆ ØªÙŠØªØ¨Ø§ÙƒØ§Ùˆ ÙˆØ¨Ø¯ÙŠØª ØªÙ†Ù‡Ø¯Ø± Ø¨ Ù†ÙØ¹Ù„ Ø¨Ø­Ø§Ù„ Ø¥Ù„Ù‰ ØªÙ†Ù‡Ø¯ÙŠ Ø§Ù„Ù†Ø§Ø³\n",
      "Ø¨ÙƒÙŠØª ÙˆØ§Ù„Ù†Ø§Ø³ ÙƒÙ„Ù‡Ù… Ø³ÙƒØªÙˆ ÙˆØ§Ù†Ø§ Ø¬Ø§Ù†ÙŠ Ù„Ø¹Ø¬Ø¨ ÙØ±Ø§Ø³ÙŠ Ø¨Ø­Ø§Ù„ Ø¥Ù„Ù‰ ÙƒÙ†Ø­Ù„Ù…\n",
      "Ù†Ø§Ø¶ ÙƒÙ„Ø´ÙŠ ÙƒÙŠØ³Ù„Ù… Ø¹Ù„ÙŠØ§ Ø£Ùˆ ÙƒÙŠÙ‡Ø¯Ø±Ùˆ Ù…Ø¹Ø§ÙŠØ§ Ù†Øµ Ø¨ÙƒÙ‰ ÙˆÙ†Øµ Ù„Ø§Ø®Ø± Ù…Ø§ÙÙ‡Ù…ØªÙˆØ´\n",
      "ØªØ¹Ø±Ø¯Ù†Ø§ Ù„Ø¹Ø´Ø§ Ùˆ ÙƒØ§Ù† Ù„Ø¹Ø´Ø§ ÙƒØ§ÙÙŠ Ù„ÙƒÙ„Ø´ÙŠ Ø§Ù„Ù…Ø¹Ø±ÙˆØ¯ÙŠÙ†\n",
      "Ø¯ÙˆØ²Ù†Ø§ ÙŠØ§Ù…Ø§Øª Ø²ÙˆÙŠÙ†Ø© Ù…Ø§ Ø¹Ù…Ø±ÙŠ Ù…Ø§ Ù†Ù†Ø³Ø§Ù‡Ø§\n",
      "ÙƒÙ…Ù„Øª Ù‚Ø±Ø§ÙŠØªÙŠ ÙÙŠ Ø§Ù„Ù…Ø¹Ù‡Ø¯ Ø¨Ø²Ø²\n",
      "Ù…Ø§ÙƒÙ†ØªØ´ ÙƒØ§Ù† Ù‚Ø±Ø§ Ù…Ø²ÙŠØ§Ù†\n",
      "Ø®Ø¯Ù…Øª ÙÙŠ ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„Ø³Ø¨ÙŠØ·Ø§Ø±Ø§Øª Ù„ÙŠ Ù‚Ø±Ø§Ø¨ Ù…Ù† Ø§Ù„Ø¯Ø§Ø± Ø§Ù„Ø­Ù…Ø¯ Ù„Ù„Ù‡ Ø£Ù…ÙˆØ±ÙŠ Ù…ØªÙŠØ³Ø±Ø© Ùˆ ÙƒÙ†Ø¹ÙŠØ´ Ù…Ø¹ ÙˆØ§Ù„Ø¯ÙŠÙ‰\n",
      "Ù‚Ø±Ø±Øª Ù†Ø¬Ù…Ø¹ Ø§Ù„ØµØ¯Ø§Ù‚ Ù„Ù…Ø±Ø§ØªÙŠ ÙƒØ§Ù†Øª Ø§Ù„Ø®Ø¯Ù…Ø© ØºØ§Ø¯Ø© Ù…Ø²ÙŠØ§Ù† Ùˆ Ø­ÙŠØª \n"
     ]
    }
   ],
   "source": [
    "s = data[:500]\n",
    "# convert tensor to array\n",
    "s = s.numpy()\n",
    "print(f'Tensor data: {decode(s)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
